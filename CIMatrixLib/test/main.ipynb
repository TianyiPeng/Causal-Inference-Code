{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.metrics import r2_score\n",
    "import pickle\n",
    "import time\n",
    "from scipy.optimize import fsolve\n",
    "from scipy.optimize import least_squares\n",
    "from scipy.optimize import minimize\n",
    "from scipy import sparse\n",
    "import os.path\n",
    "from scipy.interpolate import splrep, splev\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "from CIMatrixLib.src.util import *\n",
    "from CIMatrixLib.src.TreatPattern import *\n",
    "from CIMatrixLib.src.algorithms.DebiasConvex import *\n",
    "from CIMatrixLib.src.algorithms.CovariancePCA import *\n",
    "from CIMatrixLib.src.algorithms.MCNNM import *\n",
    "from CIMatrixLib.src.algorithms.RobustSyntheticControl import *\n",
    "import CIMatrixLib.src.readData as readData\n",
    "from CIMatrixLib.src.algorithms.OLS import *\n",
    "from CIMatrixLib.src.algorithms.SDID import *\n",
    "import importlib\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/algorithms/SDID.py\n",
    "%run ../src/algorithms/DebiasConvexMultipleTreatment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the low-rank matrix $M_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_M0(n1=50, n2=50, mean_M = 1, r = 10, gamma_shape = 1, gamma_scale = 2, type = 'Gamma'): \n",
    "    '''\n",
    "        generate a random rank-r non-negative (n1 x n2) matrix with mean(M) = mean_M\n",
    "    '''\n",
    "    if (type == 'Gamma'):\n",
    "        U = np.random.gamma(shape = gamma_shape, scale = gamma_scale, size = (n1, r))\n",
    "        V = np.random.gamma(shape = gamma_shape, scale = gamma_scale, size = (n2, r))\n",
    "        M0 = U.dot(V.T)\n",
    "        M0 = M0 / np.mean(M0) * mean_M\n",
    "    else:\n",
    "        if (type == 'Gaussian'):\n",
    "            U = np.random.normal(loc=0, scale = 1, size = (n1, r))\n",
    "            V = np.random.normal(loc = 0, scale = 1, size = (n2, r))\n",
    "            M0 = mean_M * U.dot(V.T)\n",
    "    return M0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate treatment patterns $Z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_Z(pattern_tuple = ['adaptive'], M0 = 0):\n",
    "    '''\n",
    "        generate the binary matrix Z for different patterns \n",
    "    '''\n",
    "    while (True):\n",
    "        if (pattern_tuple[0] == 'adaptive'):\n",
    "            a = pattern_tuple[1][0]\n",
    "            b = pattern_tuple[1][1]\n",
    "            Z = adpative_treatment_pattern(a, b, M0)\n",
    "    \n",
    "        if (pattern_tuple[0] == 'iid'):\n",
    "            p_treat = np.random.rand()*0.5\n",
    "            Z = np.random.rand(n1, n2) <= p_treat\n",
    "\n",
    "        if (pattern_tuple[0] == 'block'):\n",
    "            m2 = pattern_tuple[1][1]\n",
    "            Z, treat_units = simultaneous_adoption(pattern_tuple[1][0], m2, M0)\n",
    "        \n",
    "        if (pattern_tuple[0] == 'stagger'):\n",
    "            m2 = pattern_tuple[1][1]\n",
    "            Z = stagger_adoption(pattern_tuple[1][0], m2, M0)\n",
    "\n",
    "        ## if some row or some column is all treated; or Z=0; generate Z again  \n",
    "        if (np.sum(np.sum(1-Z, axis=0) == 0) > 0 or np.sum(np.sum(1-Z, axis=1) == 0) > 0 or np.sum(Z)==0): \n",
    "            if (pattern_tuple[0] == 'adaptive'):\n",
    "                return Z, 'fail'\n",
    "            continue\n",
    "        break\n",
    "    if (pattern_tuple[0] == 'block'):\n",
    "        return Z, treat_units\n",
    "    if (pattern_tuple[0] == 'adaptive'):\n",
    "        return Z, 'success'\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run different type of algorithms\n",
    "\n",
    "Algorithms:\n",
    "\n",
    "- convex_debias: our algorithm\n",
    "- convex: our algorithm without de-bias procedure\n",
    "- non-convex: least square solved by alternating minimization\n",
    "- missing: MC-NNM \n",
    "- no-fixed missing: MC-NNM without fixed effects\n",
    "- PCA: Xiong-Pelger 19\n",
    "- robust-synethtic_control: RSC\n",
    "- trivial: (average over Z) - (average over 1-Z)\n",
    "- OLS: DID\n",
    "- ideal: assume the counterfactual is known\n",
    "- SDID: SDID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_algo(algo_list, O, Z, suggest_l=-1, suggest_r=-1, de_mean_O=False, eps = 1e-3, treat_units = [], tau_star = 0, m2 = 0, M0 = 0, real_data = False, suggest_random_tau=0):\n",
    "    results = {}\n",
    "    for (index, algo) in enumerate(algo_list):\n",
    "        if (algo == 'convex_debias'):\n",
    "            M, tau, M_no_debias, tau_no_debias = tune_convex_algorithm_with_rank(O, Z, suggest_lambda = suggest_l, suggest_r=suggest_r, de_mean_O=de_mean_O, eps = eps, real_data = real_data)\n",
    "            \n",
    "        if (algo == 'convex'):\n",
    "            if (tau_no_debias == None):\n",
    "                raise Exception('do not run convex before convex_debias!')\n",
    "            M = M_no_debias\n",
    "            tau = tau_no_debias\n",
    "            #M, tau = tune_convex_algorithm_with_rank(O, Z, suggest_r = 1, suggest_lambda = -1, debias_flag=True)\n",
    "\n",
    "        if (algo == 'non_convex'):\n",
    "            M, tau, info = non_convex_algorithm(O, Z, r=suggest_r, tau = suggest_random_tau)\n",
    "        \n",
    "        if (algo == 'missing'):\n",
    "            fixed_effects = True\n",
    "            M, a, b, tau = tune_missing_algorithm_with_rank(O, 1-Z, fixed_effects=True, suggest_r = suggest_r, suggest_lambda = suggest_l, real_data = real_data)\n",
    "            one_row = np.ones((1, M.shape[1]))\n",
    "            one_col = np.ones((M.shape[0], 1))\n",
    "            if (fixed_effects):\n",
    "                M = M + a.dot(one_row) + one_col.dot(b.T)\n",
    "\n",
    "        if (algo == 'no-fixed missing'):\n",
    "            M, a, b, tau = tune_missing_algorithm_with_crossing_validation(O, 1-Z, fixed_effects=False)\n",
    "            \n",
    "        if (algo == 'PCA'):\n",
    "            M, tau = covariance_PCA(O, 1-Z, suggest_r=suggest_r)\n",
    "            M_1, tau_1 = covariance_PCA(O, 1-Z, suggest_r=1) #make the results more robust\n",
    "            if (abs(tau_1-tau_star) < abs(tau - tau_star)):\n",
    "                M = M_1\n",
    "                tau = tau_1\n",
    "                        \n",
    "        if (algo == 'robust_synthetic_control'):  \n",
    "            if (treat_units == []):\n",
    "                M, tau = stagger_pattern_RSC(O, Z, suggest_r = suggest_r)\n",
    "            else:\n",
    "                M, tau = synthetic_control(O, suggest_r=suggest_r, treat_units=treat_units, starting_time=m2)\n",
    "            #M_1, tau_1 = synthetic_control(O, suggest_r=1, treat_units=treat_units, starting_time=m2) #make the results more robust\n",
    "            #if (abs(tau_1-tau_star) < abs(tau - tau_star)):\n",
    "            #    M = M_1\n",
    "            #    tau = tau_1\n",
    "        \n",
    "        if (algo == 'trivial'):\n",
    "            tau = np.sum(O*Z)/np.sum(Z) - np.sum(O*(1-Z))/np.sum(1-Z)\n",
    "            M = O - Z * tau\n",
    "\n",
    "        if (algo == 'OLS'):\n",
    "            M, tau = OLS(O, Z, tau_star = tau_star)\n",
    "\n",
    "        if (algo == 'ideal'):\n",
    "            M = M0\n",
    "            tau = np.sum((O-M0)*Z) / np.sum(Z)\n",
    "\n",
    "        if (algo == 'SDID'):\n",
    "            if (treat_units == []):\n",
    "                M = M0 \n",
    "                treat_units = np.arange(Z.shape[0])[(Z[:, -1] == 1)]\n",
    "                starting_time = Z.shape[1] - np.min(np.sum(Z[treat_units, :], axis=1))\n",
    "\n",
    "                tau = SDID(O, Z, treat_units = treat_units, starting_time = m2)\n",
    "            else:\n",
    "                M = M0\n",
    "                tau = SDID(O, Z, treat_units = treat_units, starting_time = m2)\n",
    "\n",
    "\n",
    "\n",
    "        results[algo] = (M, tau)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-synthetic experiments on Sales data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "M0 = readData.read_data('sales')\n",
    "s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "#print(s)\n",
    "sigma = 0\n",
    "suggest_r = 35\n",
    "\n",
    "def sales_experiment_performance_run_results(num_experiment=1, sigma = 0.1, pattern = 'block', suggest_r = 10):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'OLS', 'PCA']\n",
    "\n",
    "    if (pattern == 'block'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "    datas = np.zeros((num_experiment, len(algo_list)))\n",
    "\n",
    "    (n1, n2) = M0.shape\n",
    "\n",
    "    suggest_l = -1\n",
    "    if (suggest_r != -1):\n",
    "        s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "        suggest_l = s[suggest_r]*1.1\n",
    "\n",
    "    for T in range(num_experiment):\n",
    "        if (T % 100 == 0):\n",
    "            print(time.time() - t1)\n",
    "            print('experiment ', T)\n",
    "\n",
    "        ## generating stagger pattern Z\n",
    "        if (pattern == 'stagger'):\n",
    "            m1 = np.random.randint(low=1, high=n1)\n",
    "            m2 = np.random.randint(low=int(n2/5), high=n2)\n",
    "            Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "            treat_units = []\n",
    "\n",
    "        if (pattern == 'block'):\n",
    "            m1 = np.random.randint(low=1, high=int(n1/3))\n",
    "            m2 = np.random.randint(low=int(n2/2), high=n2)\n",
    "            #m1 = 8\n",
    "            #m2 = 10\n",
    "            Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "\n",
    "        if (pattern == 'adaptive'):\n",
    "            while True:\n",
    "                a = np.random.randint(21)+5\n",
    "                b = np.random.randint(21)+5\n",
    "                Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "                if (info == 'fail'):\n",
    "                    continue\n",
    "                break\n",
    "        print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "        tau_star_o = np.mean(M0)/5\n",
    "\n",
    "        E = np.random.normal(loc=0, scale=sigma, size=M0.shape)\n",
    "\n",
    "        def test():\n",
    "            delta = np.random.normal(loc = 0, scale = tau_star_o/2, size = (n1, 1)) * np.ones((n1, n2))\n",
    "            #print(delta)\n",
    "            d1 = np.sum(Z * delta) / np.sum(Z)\n",
    "            delta = delta - d1\n",
    "            tau_star = tau_star_o + d1\n",
    "\n",
    "            O = M0 + Z*delta + tau_star * Z + E     \n",
    "\n",
    "            #O = M0 + Z * tau_star + E\n",
    "            E_op = np.linalg.norm(E + Z*delta, ord=2)\n",
    "\n",
    "            results = run_algo(algo_list, O, Z, suggest_r = suggest_r, suggest_l = suggest_l, eps = 1e-1, de_mean_O=False, treat_units=[], tau_star = tau_star, m2 = 0, M0 = M0, real_data = True)\n",
    "            \n",
    "            error_metric = {}\n",
    "            for algo in algo_list:\n",
    "                (M, tau) = results[algo]\n",
    "                error_metric[algo] = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "            return error_metric\n",
    "\n",
    "        error_metric = test()\n",
    "        print(error_metric)\n",
    "        for index, algo in enumerate(algo_list):\n",
    "                datas[T, index] = error_metric[algo]\n",
    "        print('experiment {}, time elapses '.format(T), time.time() - t1)\n",
    "    datas = pd.DataFrame(datas, columns = algo_list)\n",
    "    return datas\n",
    "\n",
    "\n",
    "np.random.seed(1)\n",
    "datas = sales_experiment_performance_run_results(sigma = sigma, num_experiment = 100, pattern = 'adaptive', suggest_r = suggest_r)\n",
    "directory = 'results/plot_results/'\n",
    "file_name = 'sales_adaptive_performance_results.p'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "    print(f\"Directory '{directory}' created.\")\n",
    "full_path = os.path.join(directory, file_name)\n",
    "pickle.dump(datas, open(full_path, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_experiment_performance_plot_results(datas, label = 'min', file_name = ''):\n",
    "    results = np.abs(datas.values)\n",
    "    results_original = datas.values\n",
    "    display(datas.describe())\n",
    "    display((datas/np.mean(M0)*5).abs().describe())\n",
    "    if (label == 'min'):\n",
    "        results = (results.T / np.min(results, axis=1)).T\n",
    "        results = np.where(results > 10, 10, results)\n",
    "    else:\n",
    "        #results = (results.T / np.max(results, axis=1)).T\n",
    "        results = results\n",
    "\n",
    "    algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA']\n",
    "    legend_dic = {'convex_debias': 'De-biased Convex', 'missing': 'MC-NNM [ABDIK18]', 'OLS':'OLS', 'PCA':'W-PCA [XP19]', 'robust_synthetic_control':'RSC'}\n",
    "    color_dic = {'convex_debias': 'blue', 'missing': 'tab:red', 'OLS':'tab:green', 'PCA':'tab:purple', 'robust_synthetic_control':'gold'}\n",
    "    legend_list = []\n",
    "    for algo in algo_list:\n",
    "        if (algo not in datas.columns):\n",
    "            algo_list.remove(algo)\n",
    "    for algo in algo_list:\n",
    "        legend_list.append(legend_dic[algo])\n",
    "\n",
    "    columns = datas.columns\n",
    "    m = len(columns)\n",
    "    comparison_table = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ai = algo_list[i]\n",
    "            aj = algo_list[j]\n",
    "            comparison_table[i][j] = np.sum(datas.abs()[ai]<datas.abs()[aj]) / datas.shape[0]\n",
    "    df_compare = pd.DataFrame(data=comparison_table, index=legend_list, columns=legend_list)\n",
    "    display(df_compare)\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "    df_list_original = []\n",
    "    for i in range(results.shape[0]):\n",
    "        for algo in algo_list:\n",
    "            j = list(datas.columns).index(algo)\n",
    "            df_list.append([results[i,j] / np.mean(M0) * 10, algo])\n",
    "\n",
    "            df_list_original.append([results_original[i,j], algo])\n",
    "\n",
    "    df_list.append([1, 'convex_debias'])\n",
    "\n",
    "    label = r'$|\\tau - \\tau^{*}| / \\tau^{*}$'\n",
    "    df = pd.DataFrame(df_list, columns = [label, 'algos']) \n",
    "    df_original = pd.DataFrame(df_list_original, columns = [label, 'algos'])\n",
    "\n",
    "\n",
    "    #g = sns.displot(data = df, x = label, hue = 'algos', hue_order = algo_list, multiple = 'dodge', shrink = 0.8, legend=False, stat = 'probability', palette=color_dic)\n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    \n",
    "\n",
    "    g = sns.displot(data = df, x = label, hue = 'algos', kind='ecdf', legend = False)\n",
    "\n",
    "    Ax = g.ax\n",
    "    Boxes = [item for item in Ax.get_children()\n",
    "        if isinstance(item, matplotlib.lines.Line2D)]\n",
    "\n",
    "    color_set = []\n",
    "    for item in Boxes:\n",
    "       if (item.get_color() in color_set):\n",
    "           pass\n",
    "       else:\n",
    "           color_set.append(item.get_color())\n",
    "    print(color_set)\n",
    "\n",
    "    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\n",
    "                 C, L in zip(color_set[::-1],\n",
    "                             legend_list)]\n",
    "\n",
    "    plt.legend(handles=legend_patches, fontsize = 13)\n",
    "    plt.ylabel('CDF', fontsize=14)\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.xlim((0, 1))\n",
    "    plt.savefig(file_name + '_cdf.eps')\n",
    "    plt.show()\n",
    "\n",
    "    sns.catplot(data = df_original, x = 'algos', y = label, kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    # for index, algo in enumerate(datas.columns):\n",
    "    #     #hist, bined = np.histogram(results[:, index], bins = 10, range = (0, 10), density=True)\n",
    "    #     #plt.plot((bined[:-1]/2+bined[1:]/2), hist, label = algo)\n",
    "    #     plt.hist(results[:, index], bins = 20, range = (0, 1), density=True, label = algo, alpha = 0.7)\n",
    "\n",
    "    # plt.xlabel('instance score')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "M0 = readData.read_data('sales')\n",
    "file_name =  'results/plot_results/sales_adaptive_performance_results'\n",
    "#file_name = 'tmp'\n",
    "datas = pickle.load(open(file_name + '.p', 'rb'))\n",
    "#synthetic_experiment_performance_plot_results(datas, label = 'min')\n",
    "sales_experiment_performance_plot_results(datas, label = 'max', file_name = file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M0 = readData.read_data('sales')\n",
    "s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "plt.plot(s)\n",
    "plt.show()\n",
    "plt.plot(np.log(s))\n",
    "print(np.sum(s[0:35])/np.sum(s))\n",
    "print(M0.shape)\n",
    "print(np.mean(M0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_beer_pattern():\n",
    "    M0 = readData.read_data('sales')\n",
    "    a = 10\n",
    "    b = 10\n",
    "    Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "    plt.imshow(Z, cmap = 'Greys', interpolation='nearest')\n",
    "    plt.savefig('results/plot_results/sales_adaptive_pattern.png', dpi=300)\n",
    "    plt.show()\n",
    "    print(convex_condition_test(M0, Z, 35))\n",
    "    PTperpZ = projection_T_orthogonal(Z, M0)\n",
    "    print(np.sum(PTperpZ**2))\n",
    "    print(np.sum(Z**2))\n",
    "plot_beer_pattern()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-synthetic experiments on Tobacco data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tobacco_experiment_performance_run_results(num_experiment=1, sigma = 0.1, pattern = 'block', suggest_r = 10, row_specific=False):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'OLS', 'PCA', 'm_debias']\n",
    "\n",
    "    if (pattern == 'block' or pattern == 'stagger'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "    #if (pattern == 'block' or pattern == 'stagger'):\n",
    "    #    algo_list.append('SDID')\n",
    "\n",
    "    datas = np.zeros((num_experiment, len(algo_list)))\n",
    "\n",
    "    (n1, n2) = M0.shape\n",
    "\n",
    "    for T in range(num_experiment):\n",
    "        if (T % 100 == 0):\n",
    "            print(time.time() - t1)\n",
    "            print('experiment ', T)\n",
    "\n",
    "        ## generating stagger pattern Z\n",
    "        if (pattern == 'stagger'):\n",
    "            m1 = np.random.randint(low=1, high=n1)\n",
    "            m2 = np.random.randint(low=1, high=n2)\n",
    "            Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "            treat_units = []\n",
    "\n",
    "        if (pattern == 'block'):\n",
    "            #m1 = np.random.randint(low=1, high=int(n1/3))\n",
    "            #m2 = np.random.randint(low=int(n2/2), high=n2)\n",
    "            m1 = np.random.randint(low=1, high=5)\n",
    "            #m2 = np.random.randint(low=1, high=n2)\n",
    "            #m1 = 8\n",
    "            #m2 = 19\n",
    "            m2 = 18\n",
    "            Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "            print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "        tau_star_o = np.mean(M0) / 5\n",
    "\n",
    "        E = np.random.normal(loc=0, scale=sigma, size=M0.shape)\n",
    "\n",
    "        plt.imshow(Z, cmap = 'Greys', interpolation='nearest')\n",
    "        plt.show()\n",
    "\n",
    "        def test():\n",
    "\n",
    "            #M0, M1, E = synthetic_intervention_pattern()\n",
    "            delta = np.random.normal(loc = 0, scale = tau_star_o/2, size = (n1, 1)) * np.ones((n1, n2))\n",
    "            #print(delta)\n",
    "            d1 = np.sum(Z * delta) / np.sum(Z)\n",
    "            delta = delta - d1\n",
    "            tau_star = tau_star_o + d1\n",
    "\n",
    "            O = M0 + Z*delta + tau_star * Z + E \n",
    "            #tau_star = np.sum(Z * (M1 - M0)) / np.sum(Z)\n",
    "            E_op = np.linalg.norm(Z*delta, ord=2)\n",
    "            s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "            print('M_0 sigma_min', s[suggest_r-1], 'Delta_norm', E_op)\n",
    "            #O = M0 + Z * tau_star + E\n",
    "            #E_op = np.linalg.norm(E, ord=2)\n",
    "\n",
    "\n",
    "            suggest_l = -1\n",
    "            if (suggest_r != -1):\n",
    "                s = np.linalg.svd(M0+E, full_matrices=False, compute_uv=False)\n",
    "                suggest_l = s[suggest_r]*1.1\n",
    "\n",
    "            results = run_algo(algo_list, O, Z, suggest_r = suggest_r, suggest_l = suggest_l, eps = 1e-4, de_mean_O=False, treat_units=treat_units, tau_star = tau_star, m2 = m2, M0 = M0, real_data = True, suggest_random_tau = tau_star*10)\n",
    "\n",
    "            if (row_specific==True):\n",
    "                \n",
    "                tau_star_i = ((tau_star + delta)*Z)[:, -1]\n",
    "            \n",
    "                error_metric = {}\n",
    "                for algo in algo_list:\n",
    "                    if (algo == 'm_debias'):\n",
    "                        continue\n",
    "                    (M, tau) = results[algo]\n",
    "                    tau_i = np.sum((O - M)*Z, axis = 1) / (np.sum(Z, axis=1) + 1e-7)\n",
    "                    error_metric[algo] = np.linalg.norm(tau_i-tau_star_i) / np.linalg.norm(tau_star_i) #metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "                    #print(np.linalg.norm(M-M0) / np.linalg.norm(M0))\n",
    "\n",
    "                s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "                suggest_l = s[suggest_r]*1.1\n",
    "\n",
    "                M, tau, info = convex_algorithm_row_specific_treatments(O, np.ones_like(O), Z, suggest_l, suggest = [], eps = 1e-3, debug = False)\n",
    "                tau_debias = debias_row_specific(M, tau, Z, suggest_l)\n",
    "                error_metric['m_debias'] = np.linalg.norm(tau_debias.reshape(-1) - tau_star_i) / np.linalg.norm(tau_star_i)\n",
    "\n",
    "            else:\n",
    "\n",
    "                error_metric = {}\n",
    "                for algo in algo_list:\n",
    "                    (M, tau) = results[algo]\n",
    "                    error_metric[algo] = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "\n",
    "            \n",
    "            return error_metric\n",
    "\n",
    "        error_metric = test()\n",
    "        print(error_metric)\n",
    "        for index, algo in enumerate(algo_list):\n",
    "                datas[T, index] = error_metric[algo]\n",
    "        print('experiment {}, time elapses '.format(T), time.time() - t1)\n",
    "    datas = pd.DataFrame(datas, columns = algo_list)\n",
    "    return datas\n",
    "\n",
    "M0 = readData.read_data('tobacco')\n",
    "s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "#print(s)\n",
    "sigma = 0\n",
    "suggest_r = 5\n",
    "\n",
    "np.random.seed(5)\n",
    "\n",
    "datas = tobacco_experiment_performance_run_results(sigma = sigma, num_experiment = 1, pattern = 'stagger', suggest_r = suggest_r, row_specific=True)\n",
    "#file_name = 'results/plot_results/tobacco_block_performance_results_r{}.p'.format(suggest_r)\n",
    "file_name = 'tmp.p'\n",
    "pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tobacco_experiment_performance_plot_results(datas, label = 'min', file_name = ''):\n",
    "    results = np.abs(datas.values)\n",
    "    results_original = datas.values\n",
    "    display(datas.describe())\n",
    "    #display(datas.abs().describe())\n",
    "    datas_abs = datas.abs()/np.mean(M0)*10\n",
    "    #datas_abs = datas_abs.where(datas_abs<50, 50)\n",
    "    display(datas_abs.describe())\n",
    "    if (label == 'min'):\n",
    "        results = (results.T / np.min(results, axis=1)).T\n",
    "        results = np.where(results > 10, 10, results)\n",
    "    else:\n",
    "        #results = (results.T / np.max(results, axis=1)).T\n",
    "        results = results\n",
    "\n",
    "    algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA']\n",
    "    legend_dic = {'convex_debias': 'De-biased Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC'}\n",
    "    color_dic = {'convex_debias': 'blue', 'missing': 'tab:red', 'OLS':'tab:green', 'PCA':'tab:purple', 'robust_synthetic_control':'gold'}\n",
    "    legend_list = []\n",
    "    for algo in algo_list:\n",
    "        if (algo not in datas.columns):\n",
    "            algo_list.remove(algo)\n",
    "    for algo in algo_list:\n",
    "        legend_list.append(legend_dic[algo])\n",
    "\n",
    "    columns = datas.columns\n",
    "    m = len(columns)\n",
    "    comparison_table = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ai = algo_list[i]\n",
    "            aj = algo_list[j]\n",
    "            comparison_table[i][j] = np.sum(datas.abs()[ai]<datas.abs()[aj]) / datas.shape[0]\n",
    "    df_compare = pd.DataFrame(data=comparison_table, index=legend_list, columns=legend_list)\n",
    "    display(df_compare)\n",
    "\n",
    "   \n",
    "    df_list = []\n",
    "    df_list_original = []\n",
    "    for i in range(results.shape[0]):\n",
    "        for algo in algo_list:\n",
    "            j = list(datas.columns).index(algo)\n",
    "            df_list.append([results[i,j]/np.mean(M0)*10, algo])\n",
    "\n",
    "            df_list_original.append([results_original[i,j], algo])\n",
    "\n",
    "    label = r'$|\\tau - \\tau^{*}| / \\tau^{*}$'\n",
    "    df = pd.DataFrame(df_list, columns = [label, 'algos'])\n",
    "    df_original = pd.DataFrame(df_list_original, columns = [label, 'algos'])\n",
    "\n",
    "    #g = sns.displot(data = df, x = label, hue = 'algos', hue_order = algo_list, multiple = 'dodge', shrink = 0.8, legend=False, stat = 'probability', palette=color_dic)\n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    \n",
    "\n",
    "    g = sns.displot(data = df, x = label, hue = 'algos', kind='ecdf', legend = False)\n",
    "\n",
    "    Ax = g.ax\n",
    "    Boxes = [item for item in Ax.get_children()\n",
    "        if isinstance(item, matplotlib.lines.Line2D)]\n",
    "\n",
    "    color_set = []\n",
    "    for item in Boxes:\n",
    "       if (item.get_color() in color_set):\n",
    "           pass\n",
    "       else:\n",
    "           color_set.append(item.get_color())\n",
    "    print(color_set)\n",
    "\n",
    "    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\n",
    "                 C, L in zip(color_set[::-1],\n",
    "                             legend_list)]\n",
    "\n",
    "    plt.legend(handles=legend_patches, fontsize = 13)\n",
    "    plt.ylabel('CDF', fontsize=14)\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.xlim((0, 1))\n",
    "    plt.savefig(file_name + '_cdf.eps')\n",
    "    plt.show()\n",
    "\n",
    "    sns.catplot(data = df_original, x = 'algos', y = label, kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    # for index, algo in enumerate(datas.columns):\n",
    "    #     #hist, bined = np.histogram(results[:, index], bins = 10, range = (0, 10), density=True)\n",
    "    #     #plt.plot((bined[:-1]/2+bined[1:]/2), hist, label = algo)\n",
    "    #     plt.hist(results[:, index], bins = 20, range = (0, 1), density=True, label = algo, alpha = 0.7)\n",
    "\n",
    "    # plt.xlabel('instance score')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "M0 = readData.read_data('tobacco')\n",
    "#file_name =  'results/plot_results/tobacco_block_performance_results_r5'\n",
    "file_name = 'tmp'\n",
    "datas = pickle.load(open(file_name + '.p', 'rb'))\n",
    "#synthetic_experiment_performance_plot_results(datas, label = 'min')\n",
    "tobacco_experiment_performance_plot_results(datas, label = 'max', file_name = file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "M0 = readData.read_data('tobacco')\n",
    "s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "plt.plot(s)\n",
    "print(np.sum(s[:4])/np.sum(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic experiments for various algorithms performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up\n",
    "n1 = 100\n",
    "n2 = 100\n",
    "mean_M = 10\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_run_results(n1 = 50, n2 = 50, mean_M = 1, num_experiment=1, sigma = 0.1, pattern = 'block'):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'OLS', 'PCA']\n",
    "\n",
    "    if (pattern == 'block' or pattern == 'stagger'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "    if (pattern == 'block'):\n",
    "        algo_list.append('SDID')\n",
    "\n",
    "    datas = np.zeros((num_experiment, len(algo_list)))\n",
    "\n",
    "\n",
    "    for T in range(num_experiment):\n",
    "        if (T % 100 == 0):\n",
    "            print(time.time() - t1)\n",
    "            print('experiment ', T)\n",
    "\n",
    "        #r = np.random.randint(low=1, high=10+1)\n",
    "        r = 10\n",
    "        M0 = synthetic_M0(n1, n2, mean_M, r, type='Gamma')\n",
    "        ## generating stagger pattern Z\n",
    "        treat_units = []\n",
    "        m1 = 0\n",
    "        m2 = 0\n",
    "        if (pattern == 'stagger'):\n",
    "            m1 = np.random.randint(low=1, high=n1)\n",
    "            #m1 = 2\n",
    "            m2 = np.random.randint(low=1, high=n2)\n",
    "            Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "            treat_units = []\n",
    "\n",
    "        if (pattern == 'block'):\n",
    "            m1 = np.random.randint(low=1, high=int(n1/10))\n",
    "            m2 = np.random.randint(low=1, high=int(n2/3))\n",
    "            #m1 = 1\n",
    "            #m2 = int(n2*0.7)\n",
    "            Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "            print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "        if (pattern == 'adaptive'):\n",
    "            while True:\n",
    "                a = np.random.randint(25)+5\n",
    "                b = np.random.randint(25)+5\n",
    "                Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "                if (info == 'fail'):\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        tau_star_o = 1\n",
    "\n",
    "        #PTperpZ = projection_T_orthogonal(Z, M0)\n",
    "\n",
    "        #predict_sigma = sigma / np.sqrt(np.sum(PTperpZ**2))\n",
    "\n",
    "        E = np.random.normal(loc=0, scale=sigma, size=M0.shape)\n",
    "\n",
    "        #E2 =  np.random.normal(loc=0, scale=sigma*5, size=M0.shape)\n",
    "        #M1 = M0 + synthetic_M0(n1, n2, mean_M / 5, r, type='Gamma')\n",
    "        #tau_star = np.sum(Z * (M1 - M0)) / np.sum(Z)\n",
    "        #print(tau_star)\n",
    "\n",
    "        s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "        def synthetic_intervention_pattern():\n",
    "            U = np.random.normal(loc=0, scale = 1, size = (n1, r))\n",
    "            V = np.random.normal(loc = 0, scale = 1, size = (n2, r))\n",
    "            M0 = U.dot(V.T)\n",
    "\n",
    "            V = (np.random.rand(n2, r) - 0.5) * 2 * np.sqrt(3)\n",
    "            M1 = U.dot(V.T)\n",
    "\n",
    "            E = np.random.normal(loc = 0, scale = 1, size = (n1, n2))\n",
    "            return M0, M1, E\n",
    "\n",
    "        def test():\n",
    "            #M0, M1, E = synthetic_intervention_pattern()\n",
    "            delta = np.random.normal(loc = 0, scale = 1, size = (n1, 1)) * np.ones((n1, n2))\n",
    "            #print(delta)\n",
    "            d1 = np.sum(Z * delta) / np.sum(Z)\n",
    "            delta = delta - d1\n",
    "            tau_star = tau_star_o + d1\n",
    "\n",
    "            O = M0 + Z*delta + tau_star * Z + E \n",
    "            #tau_star = np.sum(Z * (M1 - M0)) / np.sum(Z)\n",
    "            E_op = np.linalg.norm(E+Z*delta, ord=2)\n",
    "            s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "            suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "            print('E norm', np.linalg.norm(E, ord=2), 'delta_norm', np.linalg.norm(Z*delta), 'M0_s', s[r-1])\n",
    "            print(s[r-1], E_op, np.linalg.norm(E, ord=2))\n",
    "\n",
    "            #O = M0 + Z * tau_star + Z * E2 + E\n",
    "            #E_op = np.linalg.norm(E+Z*E2, ord=2)\n",
    "            #suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "\n",
    "            #O = (1-Z) * M0 + Z * M1 + E\n",
    "            #E_op = np.linalg.norm(E+Z*(M1-M0-tau_star), ord=2)\n",
    "            #print(s[r-1], E_op, np.linalg.norm(E, ord=2), np.linalg.norm(Z*(M1-M0-tau_star), ord=2))\n",
    "            #suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "            \n",
    "            # s1 = np.linalg.svd(M0 + E, full_matrices=False, compute_uv=False)\n",
    "            # try_r = 15\n",
    "            # suggest_l = s1[try_r-1]/1.1\n",
    "\n",
    "            results = run_algo(algo_list, O, Z, suggest_r = r, suggest_l = suggest_l, eps = 1e-4, de_mean_O=False, treat_units=treat_units, tau_star = tau_star, m2 = m2, M0 = M0)\n",
    "            \n",
    "            error_metric = {}\n",
    "            for algo in algo_list:\n",
    "                (M, tau) = results[algo]\n",
    "                error_metric[algo] = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "            return error_metric\n",
    "\n",
    "        error_metric = test()\n",
    "        print(error_metric)\n",
    "        for index, algo in enumerate(algo_list):\n",
    "                datas[T, index] = error_metric[algo]\n",
    "        print('experiment {}, time elapses '.format(T), time.time() - t1)\n",
    "    datas = pd.DataFrame(datas, columns = algo_list)\n",
    "    return datas\n",
    "\n",
    "\n",
    "pattern = 'stagger'\n",
    "datas = synthetic_experiment_performance_run_results(n1 = n1, n2 = n2, mean_M = mean_M, sigma = sigma, num_experiment = 100, pattern = pattern)\n",
    "file_name = 'results/plot_results/synthetic_{}_m1_m2_performance_results.p'.format(pattern)\n",
    "#file_name = 'tmp.p'\n",
    "pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_plot_results(datas, label = 'min', file_name = ''):\n",
    "    results = np.abs(datas.values)\n",
    "    results_original = datas.values\n",
    "    display(datas.describe())\n",
    "    display(datas.abs().describe())\n",
    "    #display((datas/10).abs().describe())\n",
    "    if (label == 'min'):\n",
    "        results = (results.T / np.min(results, axis=1)).T\n",
    "        results = np.where(results > 10, 10, results)\n",
    "    else:\n",
    "        #results = (results.T / np.max(results, axis=1)).T\n",
    "        results = results\n",
    "\n",
    "    algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA']\n",
    "    legend_dic = {'convex_debias': 'Debias Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC'}\n",
    "    color_dic = {'convex_debias': 'blue', 'missing': 'tab:red', 'OLS':'tab:green', 'PCA':'tab:purple', 'robust_synthetic_control':'gold'}\n",
    "    legend_list = []\n",
    "    for algo in algo_list:\n",
    "        if (algo not in datas.columns):\n",
    "            algo_list.remove(algo)\n",
    "    for algo in algo_list:\n",
    "        legend_list.append(legend_dic[algo])\n",
    "\n",
    "    columns = datas.columns\n",
    "    m = len(columns)\n",
    "    comparison_table = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ai = algo_list[i]\n",
    "            aj = algo_list[j]\n",
    "            comparison_table[i][j] = np.sum(datas.abs()[ai]<datas.abs()[aj]) / datas.shape[0]\n",
    "    df_compare = pd.DataFrame(data=comparison_table, index=legend_list, columns=legend_list)\n",
    "    display(df_compare)\n",
    "\n",
    "   \n",
    "    df_list = []\n",
    "    df_list_original = []\n",
    "    for i in range(results.shape[0]):\n",
    "        for algo in algo_list:\n",
    "            j = list(datas.columns).index(algo)\n",
    "            df_list.append([results[i,j], algo])\n",
    "\n",
    "            df_list_original.append([results_original[i,j], algo])\n",
    "\n",
    "    label = r'$\\frac{|\\tau - \\tau^{*}|}{\\bar{M^{*}}}$'\n",
    "    df = pd.DataFrame(df_list, columns = [label, 'algos'])\n",
    "    df_original = pd.DataFrame(df_list_original, columns = [label, 'algos'])\n",
    "\n",
    "    #g = sns.displot(data = df, x = label, hue = 'algos', hue_order = algo_list, multiple = 'dodge', shrink = 0.8, legend=False, stat = 'probability', palette=color_dic)\n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    \n",
    "\n",
    "    g = sns.displot(data = df, x = label, hue = 'algos', kind='ecdf', legend = False)\n",
    "\n",
    "    Ax = g.ax\n",
    "    Boxes = [item for item in Ax.get_children()\n",
    "        if isinstance(item, matplotlib.lines.Line2D)]\n",
    "\n",
    "    color_set = []\n",
    "    for item in Boxes:\n",
    "       if (item.get_color() in color_set):\n",
    "           pass\n",
    "       else:\n",
    "           color_set.append(item.get_color())\n",
    "    print(color_set)\n",
    "\n",
    "    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\n",
    "                 C, L in zip(color_set[::-1],\n",
    "                             legend_list)]\n",
    "\n",
    "    plt.legend(handles=legend_patches, fontsize = 13)\n",
    "    plt.ylabel('CDF', fontsize=14)\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.xlim((0, 0.8))\n",
    "    plt.savefig(file_name + '_cdf.eps')\n",
    "    plt.show()\n",
    "\n",
    "    sns.catplot(data = df_original, x = 'algos', y = label, kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    # for index, algo in enumerate(datas.columns):\n",
    "    #     #hist, bined = np.histogram(results[:, index], bins = 10, range = (0, 10), density=True)\n",
    "    #     #plt.plot((bined[:-1]/2+bined[1:]/2), hist, label = algo)\n",
    "    #     plt.hist(results[:, index], bins = 20, range = (0, 1), density=True, label = algo, alpha = 0.7)\n",
    "\n",
    "    # plt.xlabel('instance score')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "#file_name =  'results/plot_results/synthetic_stagger_m1_m2_performance_results'\n",
    "#datas = pickle.load(open(file_name + '.p', 'rb'))\n",
    "#synthetic_experiment_performance_plot_results(datas, label = 'min')\n",
    "synthetic_experiment_performance_plot_results(datas, label = 'max', file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Row Specific Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(5, 1)\n",
    "B = np.ones((5, 7))\n",
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up\n",
    "n1 = 100\n",
    "n2 = 100\n",
    "mean_M = 10\n",
    "sigma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_run_results(n1 = 50, n2 = 50, mean_M = 1, num_experiment=1, sigma = 0.1, pattern = 'block'):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'm_debias']\n",
    "\n",
    "    if (pattern == 'block' or pattern == 'stagger'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "    #if (pattern == 'block'):\n",
    "    #    algo_list.append('SDID')\n",
    "\n",
    "    datas = np.zeros((num_experiment, len(algo_list)))\n",
    "\n",
    "\n",
    "    for T in range(num_experiment):\n",
    "        if (T % 100 == 0):\n",
    "            print(time.time() - t1)\n",
    "            print('experiment ', T)\n",
    "\n",
    "        #r = np.random.randint(low=1, high=10+1)\n",
    "        r = 10\n",
    "        M0 = synthetic_M0(n1, n2, mean_M, r, type='Gamma')\n",
    "        ## generating stagger pattern Z\n",
    "        treat_units = []\n",
    "        m1 = 0\n",
    "        m2 = 0\n",
    "        if (pattern == 'stagger'):\n",
    "            m1 = np.random.randint(low=1, high=n1)\n",
    "            #m1 = 2\n",
    "            m2 = np.random.randint(low=1, high=n2)\n",
    "            Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "            treat_units = []\n",
    "\n",
    "        if (pattern == 'block'):\n",
    "            m1 = np.random.randint(low=1, high=int(n1/3))\n",
    "            m2 = np.random.randint(low=int(n2/2), high=int(n2))\n",
    "            #m1 = 1\n",
    "            #m2 = int(n2*0.7)\n",
    "            Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "            print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "        if (pattern == 'adaptive'):\n",
    "            while True:\n",
    "                a = np.random.randint(25)+5\n",
    "                b = np.random.randint(25)+5\n",
    "                Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "                if (info == 'fail'):\n",
    "                    continue\n",
    "                break\n",
    "\n",
    "        tau_star = 1\n",
    "\n",
    "        #PTperpZ = projection_T_orthogonal(Z, M0)\n",
    "\n",
    "        #predict_sigma = sigma / np.sqrt(np.sum(PTperpZ**2))\n",
    "\n",
    "        E = np.random.normal(loc=0, scale=sigma, size=M0.shape)\n",
    "\n",
    "        #E2 =  np.random.normal(loc=0, scale=sigma*5, size=M0.shape)\n",
    "        #M1 = M0 + synthetic_M0(n1, n2, mean_M / 5, r, type='Gamma')\n",
    "        #tau_star = np.sum(Z * (M1 - M0)) / np.sum(Z)\n",
    "        #print(tau_star)\n",
    "\n",
    "        s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "        def test():\n",
    "            #M0, M1, E = synthetic_intervention_pattern()\n",
    "            delta = np.random.normal(loc = 0, scale = 1, size = (n1, 1)) * np.ones((n1, n2))\n",
    "            #print(delta)\n",
    "            #d1 = np.sum(Z * delta) / np.sum(Z)\n",
    "            #delta = delta - d1\n",
    "            #tau_star = tau_star_o + d1\n",
    "\n",
    "            O = M0 + Z*delta + tau_star * Z + E \n",
    "            #tau_star = np.sum(Z * (M1 - M0)) / np.sum(Z)\n",
    "            E_op = np.linalg.norm(E+Z*delta, ord=2)\n",
    "            s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "            suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "            print('E norm', np.linalg.norm(E, ord=2), 'delta_norm', np.linalg.norm(Z*delta), 'M0_s', s[r-1])\n",
    "            print(s[r-1], E_op, np.linalg.norm(E, ord=2))\n",
    "\n",
    "            #O = M0 + Z * tau_star + Z * E2 + E\n",
    "            #E_op = np.linalg.norm(E+Z*E2, ord=2)\n",
    "            #suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "\n",
    "            #O = (1-Z) * M0 + Z * M1 + E\n",
    "            #E_op = np.linalg.norm(E+Z*(M1-M0-tau_star), ord=2)\n",
    "            #print(s[r-1], E_op, np.linalg.norm(E, ord=2), np.linalg.norm(Z*(M1-M0-tau_star), ord=2))\n",
    "            #suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "            \n",
    "            # s1 = np.linalg.svd(M0 + E, full_matrices=False, compute_uv=False)\n",
    "            # try_r = 15\n",
    "            # suggest_l = s1[try_r-1]/1.1\n",
    "\n",
    "            results = run_algo(algo_list, O, Z, suggest_r = r, suggest_l = suggest_l, eps = 1e-4, de_mean_O=False, treat_units=treat_units, tau_star = tau_star, m2 = m2, M0 = M0)\n",
    "\n",
    "            tau_star_i = ((tau_star + delta)*Z)[:, -1]\n",
    "            \n",
    "            error_metric = {}\n",
    "            for algo in algo_list:\n",
    "                if (algo == 'm_debias'):\n",
    "                    continue\n",
    "                (M, tau) = results[algo]\n",
    "                error = 0\n",
    "                tau_i = np.sum((O - M)*Z, axis = 1) / (np.sum(Z, axis=1) + 1e-7)\n",
    "                error_metric[algo] = np.linalg.norm(tau_i-tau_star_i) #metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "                print(np.linalg.norm(M-M0) / np.linalg.norm(M0))\n",
    "\n",
    "\n",
    "            E_op = np.linalg.norm(E, ord=2)\n",
    "            s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "            suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "\n",
    "            M, tau, info = convex_algorithm_row_specific_treatments(O, np.ones_like(O), Z, suggest_l, suggest = [], eps = 1e-3, debug = False)\n",
    "            tau_debias = debias_row_specific(M, tau, Z, suggest_l)\n",
    "            error_metric['m_debias'] = np.linalg.norm(tau_debias.reshape(-1) - tau_star_i)\n",
    "            return error_metric\n",
    "\n",
    "        error_metric = test()\n",
    "        print(error_metric)\n",
    "        for index, algo in enumerate(algo_list):\n",
    "                datas[T, index] = error_metric[algo]\n",
    "        print('experiment {}, time elapses '.format(T), time.time() - t1)\n",
    "    datas = pd.DataFrame(datas, columns = algo_list)\n",
    "    return datas\n",
    "\n",
    "\n",
    "pattern = 'block'\n",
    "datas = synthetic_experiment_performance_run_results(n1 = n1, n2 = n2, mean_M = mean_M, sigma = sigma, num_experiment = 100, pattern = pattern)\n",
    "file_name = 'results/plot_results/synthetic_{}_m1_m2_performance_results.p'.format(pattern)\n",
    "#file_name = 'tmp.p'\n",
    "pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_plot_results(datas, label = 'min', file_name = ''):\n",
    "    results = np.abs(datas.values)\n",
    "    results_original = datas.values\n",
    "    display(datas.describe())\n",
    "    display(datas.abs().describe())\n",
    "    #display((datas/10).abs().describe())\n",
    "    if (label == 'min'):\n",
    "        results = (results.T / np.min(results, axis=1)).T\n",
    "        results = np.where(results > 10, 10, results)\n",
    "    else:\n",
    "        #results = (results.T / np.max(results, axis=1)).T\n",
    "        results = results\n",
    "\n",
    "    algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA']\n",
    "    legend_dic = {'convex_debias': 'Debias Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC'}\n",
    "    color_dic = {'convex_debias': 'blue', 'missing': 'tab:red', 'OLS':'tab:green', 'PCA':'tab:purple', 'robust_synthetic_control':'gold'}\n",
    "    legend_list = []\n",
    "    for algo in algo_list:\n",
    "        if (algo not in datas.columns):\n",
    "            algo_list.remove(algo)\n",
    "    for algo in algo_list:\n",
    "        legend_list.append(legend_dic[algo])\n",
    "\n",
    "    columns = datas.columns\n",
    "    m = len(columns)\n",
    "    comparison_table = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ai = algo_list[i]\n",
    "            aj = algo_list[j]\n",
    "            comparison_table[i][j] = np.sum(datas.abs()[ai]<datas.abs()[aj]) / datas.shape[0]\n",
    "    df_compare = pd.DataFrame(data=comparison_table, index=legend_list, columns=legend_list)\n",
    "    display(df_compare)\n",
    "\n",
    "   \n",
    "    df_list = []\n",
    "    df_list_original = []\n",
    "    for i in range(results.shape[0]):\n",
    "        for algo in algo_list:\n",
    "            j = list(datas.columns).index(algo)\n",
    "            df_list.append([results[i,j], algo])\n",
    "\n",
    "            df_list_original.append([results_original[i,j], algo])\n",
    "\n",
    "    label = r'$\\frac{|\\tau - \\tau^{*}|}{\\bar{M^{*}}}$'\n",
    "    df = pd.DataFrame(df_list, columns = [label, 'algos'])\n",
    "    df_original = pd.DataFrame(df_list_original, columns = [label, 'algos'])\n",
    "\n",
    "    #g = sns.displot(data = df, x = label, hue = 'algos', hue_order = algo_list, multiple = 'dodge', shrink = 0.8, legend=False, stat = 'probability', palette=color_dic)\n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    \n",
    "\n",
    "    g = sns.displot(data = df, x = label, hue = 'algos', kind='ecdf', legend = False)\n",
    "\n",
    "    Ax = g.ax\n",
    "    Boxes = [item for item in Ax.get_children()\n",
    "        if isinstance(item, matplotlib.lines.Line2D)]\n",
    "\n",
    "    color_set = []\n",
    "    for item in Boxes:\n",
    "       if (item.get_color() in color_set):\n",
    "           pass\n",
    "       else:\n",
    "           color_set.append(item.get_color())\n",
    "    print(color_set)\n",
    "\n",
    "    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\n",
    "                 C, L in zip(color_set[::-1],\n",
    "                             legend_list)]\n",
    "\n",
    "    plt.legend(handles=legend_patches, fontsize = 13)\n",
    "    plt.ylabel('CDF', fontsize=14)\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.xlim((0, 0.8))\n",
    "    plt.savefig(file_name + '_cdf.eps')\n",
    "    plt.show()\n",
    "\n",
    "    sns.catplot(data = df_original, x = 'algos', y = label, kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "    # for index, algo in enumerate(datas.columns):\n",
    "    #     #hist, bined = np.histogram(results[:, index], bins = 10, range = (0, 10), density=True)\n",
    "    #     #plt.plot((bined[:-1]/2+bined[1:]/2), hist, label = algo)\n",
    "    #     plt.hist(results[:, index], bins = 20, range = (0, 1), density=True, label = algo, alpha = 0.7)\n",
    "\n",
    "    # plt.xlabel('instance score')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "#file_name =  'results/plot_results/synthetic_stagger_m1_m2_performance_results'\n",
    "#datas = pickle.load(open(file_name + '.p', 'rb'))\n",
    "#synthetic_experiment_performance_plot_results(datas, label = 'min')\n",
    "synthetic_experiment_performance_plot_results(datas, label = 'max', file_name = file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic experiments for distribution test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up\n",
    "n1 = 100\n",
    "n2 = 100\n",
    "mean_M = 10\n",
    "r = 10\n",
    "sigma = 1\n",
    "sigma_d = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "def synthetic_experiment_distribution_run_results(n1 = 50, n2 = 50, mean_M = 1, r = 5, num_experiment=1, sigma = 0.1, sigma_d = 0.1, pattern = 'stagger'):\n",
    "    '''\n",
    "        generate (M0, Z) pair:\n",
    "            - M0 has shape (50x50) with mean_M and rank r\n",
    "            - Z is generated in a stagger way, randomly select m1 rows, each row randomly gets treated after column m2\n",
    "            - m1 ~ [1, n1), m2 ~ [n2/5, n2) uniformly\n",
    "\n",
    "        for each (M0, Z) pair:\n",
    "            - compute the score\n",
    "            -   \n",
    "\n",
    "    '''\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "    for T in range(num_experiment):\n",
    "        if (T % 100 == 0):\n",
    "            print(time.time() - t1)\n",
    "            print('experiment ', T)\n",
    "        #np.random.seed(1)\n",
    "        M0 = synthetic_M0(n1, n2, mean_M, r)\n",
    "        ## generating stagger pattern Z\n",
    "        if (pattern == 'stagger'):\n",
    "            m1 = np.random.randint(low=1, high=n1)\n",
    "            m2 = np.random.randint(low=int(n2/2), high=n2)\n",
    "            Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "\n",
    "        if (pattern == 'block'):\n",
    "            m1 = np.random.randint(low=1, high=int(n1/3))\n",
    "            m2 = np.random.randint(low=int(n2/2), high=n2)\n",
    "            Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "\n",
    "        print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "        tau_star = 1\n",
    "\n",
    "        PTperpZ = projection_T_orthogonal(Z, M0)\n",
    "\n",
    "        #predict_sigma = sigma / np.sqrt(np.sum(PTperpZ**2))\n",
    "\n",
    "        predict_sigma =  np.sqrt((sigma**2) / np.sum(PTperpZ**2) + (sigma_d**2) * np.sum((PTperpZ**2)*Z) / (np.sum(PTperpZ**2)**2))\n",
    "\n",
    "        #print(predict_sigma, sigma / np.sqrt(np.sum(PTperpZ**2)))\n",
    "\n",
    "        s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "        def test():\n",
    "            #np.random.seed(T)\n",
    "            E = np.random.normal(loc=0, scale=sigma, size=M0.shape)\n",
    "            delta = np.random.normal(loc = 0, scale = sigma_d, size = M0.shape)\n",
    "            O = M0 + Z * tau_star + E + delta * Z\n",
    "            E_op = np.linalg.norm(E + delta * Z, ord=2)\n",
    "            suggest_l = min(s[r-1]/1.1, E_op*1.1)\n",
    "\n",
    "            #input O/predict_sigma, eliminate precision issue\n",
    "            results = run_algo(['convex_debias', 'convex'], O, Z, suggest_r = -1, suggest_l = suggest_l, eps = predict_sigma/1000, de_mean_O=False)\n",
    "            (M, tau) = results['convex_debias']\n",
    "            (M_no_debias, tau_no_debias) = results['convex']\n",
    "\n",
    "\n",
    "            projected_Z = projection_T_orthogonal(Z, M_no_debias)\n",
    "\n",
    "            #E_hat = O - M_no_debias - tau_no_debias * Z\n",
    "\n",
    "            #print(np.sum(E_hat**2) / np.sum((E + delta*Z)**2))\n",
    "\n",
    "\n",
    "            \n",
    "            estimated_sigma_level1 = np.sqrt((sigma**2) / np.sum(projected_Z**2) + (sigma_d**2) * np.sum((projected_Z**2)*Z) / (np.sum(projected_Z**2)**2))\n",
    "\n",
    "            #estimated_sigma_level2 = np.sqrt(np.sum((projected_Z**2)*(E_hat**2))) / np.sum(projected_Z**2)\n",
    "\n",
    "            #print(predict_sigma, estimated_sigma_level1, estimated_sigma_level2)\n",
    "\n",
    "            return (tau-tau_star)/estimated_sigma_level1\n",
    "\n",
    "            return (tau-tau_star)/predict_sigma\n",
    "\n",
    "        # def KS_test():\n",
    "        #     total = 100\n",
    "        #     tau_samples = np.zeros(total)\n",
    "        #     for i in range(total):\n",
    "        #         tau_samples[i] = test()\n",
    "        #     KS_statistic, p_value = scipy.stats.ks_1samp(tau_samples, scipy.stats.norm.cdf)\n",
    "        #     print(KS_statistic, p_value)\n",
    "        #     return KS_statistic\n",
    "\n",
    "        samples[T] = test()\n",
    "        print('experiment {}, time elapses {}, tau error {}'.format(T, time.time() - t1, samples[T]))\n",
    "        #print(samples[T], predict_sigma)\n",
    "    return samples\n",
    "\n",
    "#for r in [2, 4, 6, 8, 10]:\n",
    "#for r in [4, 10, 15, 20, 25]:\n",
    "r = 10\n",
    "\n",
    "#samples = synthetic_experiment_distribution_run_results(n1 = n1, n2 = n2, mean_M = mean_M, r = r, sigma = sigma, sigma_d = sigma_d, num_experiment = 1000, pattern = 'stagger')\n",
    "\n",
    "\n",
    "n1_list = [50, 100, 150, 200]\n",
    "ratio_list = [0.5, 1, 2, 4]\n",
    "for n1 in n1_list:\n",
    "    for ratio in ratio_list:\n",
    "        n2 = int(n1 * ratio)\n",
    "        samples = synthetic_experiment_distribution_run_results(n1 = n1, n2 = n2, mean_M = mean_M, r = r, sigma = sigma, sigma_d = sigma, num_experiment = 1000, pattern = 'stagger')\n",
    "        file_name = 'results/plot_results/stagger_estimateT_distribution_samples_r{}_n1_{}_n2_{}.p'.format(r, n1, n2)\n",
    "        pickle.dump(samples, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_experiment_distribution_plot_distribution_results(samples, file_name):\n",
    "    hist, bined = np.histogram(samples, bins = 30, density=True)\n",
    "    plt.plot((bined[:-1]/2+bined[1:]/2), hist)\n",
    "    pos_guassian = np.linspace(min(samples), max(samples), 1000)\n",
    "    pdf_guassian = norm.pdf(pos_guassian, loc=0, scale=1)\n",
    "    plt.plot(pos_guassian, pdf_guassian)\n",
    "    plt.show()\n",
    "\n",
    "    print(np.mean(samples), np.std(samples))\n",
    "\n",
    "    g = sns.displot(data=samples, kind='hist', stat='density')\n",
    "    g.set(xlim=(-4, 4))\n",
    "    g.set(ylim=(0.0, 0.45))\n",
    "    plt.plot(pos_guassian, pdf_guassian, label=r'$N(0, 1)$', color='r')\n",
    "    plt.legend(fontsize = 17)\n",
    "    plt.ylabel('Density', fontsize = 18)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(file_name + '.eps')\n",
    "    plt.show()\n",
    "    x = scipy.stats.norm.rvs(loc=0, size=100000)\n",
    "    sns.ecdfplot(data=x)\n",
    "    plt.show()\n",
    "\n",
    "    #print(scipy.stats.wasserstein_distance(samples, x))\n",
    "\n",
    "def KS_test(samples):\n",
    "    KS_statistic, p_value = scipy.stats.ks_1samp(samples, scipy.stats.norm.cdf)\n",
    "    \n",
    "    print(KS_statistic, p_value)\n",
    "\n",
    "    x = scipy.stats.norm.rvs(loc=0, size=10000)\n",
    "    print(scipy.stats.ks_1samp(x, scipy.stats.norm.cdf))\n",
    "    return KS_statistic\n",
    "\n",
    "file_path = 'results/plot_results/block'\n",
    "    # file_name = file_path+'_distribution_before_KS_test_samples_r{}.p'.format(r)\n",
    "    # samples = pickle.load(open(file_name, 'rb'))\n",
    "synthetic_experiment_distribution_plot_distribution_results(samples, file_path + '_distribution_r{}'.format(r))\n",
    "# KS_test(samples)\n",
    "# print('0.95 portion', np.sum(np.abs(samples)<=1.96), len(samples))\n",
    "\n",
    "\n",
    "#for r in [2, 4, 6, 8, 10]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n1_list = [50, 100, 150, 200]\n",
    "ratio_list = [0.5, 1, 2, 4]\n",
    "df = pd.DataFrame()\n",
    "data = np.zeros((4, 4))\n",
    "for i,n1 in enumerate(n1_list):\n",
    "    for j,ratio in enumerate(ratio_list):\n",
    "        n2 = int(n1 * ratio)\n",
    "        file_path = 'results/plot_results/stagger'\n",
    "        file_name = 'results/plot_results/stagger_estimateT_distribution_samples_r{}_n1_{}_n2_{}.p'.format(r, n1, n2)\n",
    "        samples = pickle.load(open(file_name, 'rb'))\n",
    "        #synthetic_experiment_distribution_plot_distribution_results(samples, file_path + '_distribution_r{}'.format(r))\n",
    "        #KS_test(samples)\n",
    "        print('0.95 portion', n1, n2, np.sum(np.abs(samples)<=1.96), len(samples))\n",
    "        data[i, j] = np.sum(np.abs(samples)<=1.96)/len(samples)\n",
    "\n",
    "row_headers = [r'$n_1 = 50$', '100', '150', '200']\n",
    "column_headers = [r'$\\frac{n_2}{n_1} = 0.5$', '1', '2', '4']\n",
    "cell_text = data\n",
    "rcolors = plt.cm.BuPu(np.full(len(row_headers), 0.1))\n",
    "ccolors = plt.cm.BuPu(np.full(len(column_headers), 0.1))\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_axis_off()\n",
    "table = plt.table(cellText=cell_text,\n",
    "                      rowLabels=row_headers,\n",
    "                      rowColours=rcolors,\n",
    "                      rowLoc='right',\n",
    "                      colColours=ccolors,\n",
    "                      colLabels=column_headers,\n",
    "                      cellLoc='center')\n",
    "table.set_fontsize(14)\n",
    "table.scale(1, 6)\n",
    "plt.savefig('tmp.eps')\n",
    "#scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_r = [4, 10, 15, 20, 25]\n",
    "KS = []\n",
    "for r in array_r:\n",
    "    file_name = 'results/plot_results/block_distribution_before_KS_test_samples_r{}.p'.format(r)\n",
    "    samples = pickle.load(open(file_name, 'rb'))\n",
    "    KS.append(KS_test(samples))\n",
    "plt.plot(array_r, KS, label = 'Block Pattern', linestyle='-', marker='o')\n",
    "\n",
    "KS = []\n",
    "for r in array_r:\n",
    "    file_name = 'results/plot_results/stagger_distribution_before_KS_test_samples_r{}.p'.format(r)\n",
    "    samples = pickle.load(open(file_name, 'rb'))\n",
    "    KS.append(KS_test(samples))\n",
    "plt.plot(array_r, KS, label = 'Stagger Pattern', linestyle='-', marker='o')\n",
    "\n",
    "plt.xlabel(r'Rank $r$', fontsize = 15)\n",
    "plt.ylabel('KS distance', fontsize = 15)\n",
    "plt.legend(fontsize = 14)\n",
    "plt.savefig('results/plot_results/KS-rank-plot.eps')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution plot for K-S statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ks_critical_value(n_trials, alpha):\n",
    "    return scipy.stats.kstwo.ppf(1-alpha, n_trials)\n",
    "\n",
    "def synthetic_experiment_distribution_plot_statistic_results(samples):\n",
    "    #plt.hist(samples, bins = 10, range = (0.03, 0.2), density=True, label = 'empirical K-S statistics')\n",
    "\n",
    "    x = np.linspace(ks_critical_value(10000, 0.01), ks_critical_value(10000, 0.99), 100)\n",
    "    plt.plot(x, scipy.stats.kstwo.pdf(x, 10000), 'r-', lw=2, label = 'theoretical distribution')\n",
    "\n",
    "    plt.axvline(ks_critical_value(100, 0.05), color='r', lw=2, ls='--', alpha=0.3, label='p-value 0.05')\n",
    "    plt.axvline(ks_critical_value(100, 0.10), color='b', lw=2, ls='--', alpha=0.3, label='p-value 0.1')\n",
    "    plt.xlabel('K-S statistic over 100 samples')\n",
    "    plt.legend()\n",
    "    plt.savefig('results/plot_results/block_distribution_samples.eps')\n",
    "    plt.show()\n",
    "\n",
    "    print('empirical portion with p-value >= 0.05 is ', np.sum(samples >= ks_critical_value(100, 0.05)) / len(samples))\n",
    "    print('empirical portion with p-value >= 0.1 is ', np.sum(samples >= ks_critical_value(100, 0.1)) / len(samples))\n",
    "\n",
    "file_name = 'results/plot_results/block_distribution_samples.p'\n",
    "samples = pickle.load(open(file_name, 'rb'))\n",
    "synthetic_experiment_distribution_plot_statistic_results(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant to the changes of $\\tau^{*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/algorithms/SDID.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_run_results(n1 = 50, n2 = 50, mean_M = 1, num_experiment=1, sigma = 0.1, pattern = 'block'):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "\n",
    "    r = 10\n",
    "\n",
    "    np.random.seed(10)\n",
    "    M0 = synthetic_M0(n1, n2, mean_M, r, type='Gamma')\n",
    "    \n",
    "\n",
    "    E = np.random.normal(loc=0, scale=sigma, size=(n1, n2))\n",
    "\n",
    "    s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "    if (pattern == 'stagger'):\n",
    "        m1 = np.random.randint(low=1, high=n1)\n",
    "        m2 = np.random.randint(low=1, high=n2)\n",
    "        Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "        treat_units = []\n",
    "\n",
    "    if (pattern == 'block'):\n",
    "        m1 = np.random.randint(low=1, high=int(n1/10))\n",
    "        m2 = 20#np.random.randint(low=1, high=int(n2/3))\n",
    "        #m1 = 1\n",
    "        Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "\n",
    "    if (pattern == 'adaptive'):\n",
    "        while True:\n",
    "            a = np.random.randint(40)+5\n",
    "            b = np.random.randint(40)+5\n",
    "            #b = 2\n",
    "            Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "            if (info == 'fail'):\n",
    "                continue\n",
    "            break\n",
    "        treat_units = []\n",
    "        m2 = 0\n",
    "    \n",
    "    print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "    datas = np.zeros((len(tau_list), len(algo_list)))\n",
    "\n",
    "    for (T, tau_star) in enumerate(tau_list):\n",
    "\n",
    "        def test():\n",
    "            O = M0 + Z * tau_star + E\n",
    "            suggest_l = s[r-1]/1.1\n",
    "\n",
    "            random_tau_star = np.random.rand()*tau_star*10\n",
    "            results = run_algo(algo_list, O, Z, suggest_r = r, suggest_l = suggest_l, eps = 1e-6, de_mean_O=False, treat_units=treat_units, tau_star = tau_star, m2 = m2, M0 = M0, suggest_random_tau=0)\n",
    "            \n",
    "            error_metric = {}\n",
    "            for algo in algo_list:\n",
    "                (M, tau) = results[algo]\n",
    "                error_metric[algo] = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "            return error_metric\n",
    "\n",
    "        error_metric = test()\n",
    "        print(error_metric)\n",
    "        for index, algo in enumerate(algo_list):\n",
    "                datas[T, index] = error_metric[algo]\n",
    "        print('tau_star {}, time elapses '.format(tau_star), time.time() - t1)\n",
    "    datas = pd.DataFrame(datas, columns = algo_list)\n",
    "    return datas\n",
    "\n",
    "tau_list = np.arange(-200, 201, 50)\n",
    "mean_M = 10\n",
    "n1 = 100\n",
    "n2 = 100\n",
    "sigma = 1\n",
    "pattern = 'block'\n",
    "algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'SDID']\n",
    "\n",
    "\n",
    "datas = synthetic_experiment_performance_run_results(n1 = n1, n2 = n2, mean_M = mean_M, sigma = sigma, num_experiment = 1, pattern = pattern)\n",
    "\n",
    "legend_dic = {'convex_debias': 'Debias Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'robust_synthetic_control':'RSC', 'SDID': 'SDID'}\n",
    "\n",
    "for algo in algo_list:\n",
    "    plt.plot(tau_list, np.abs(datas[algo]), label = legend_dic[algo])\n",
    "plt.legend()\n",
    "plt.xlabel(r'$\\tau^{*}$', fontsize = 15)\n",
    "plt.ylabel(r'$|\\tau-\\tau^{*}|$', fontsize = 15)\n",
    "plt.savefig('invariance-tau.eps')\n",
    "#file_name = 'results/plot_results/synthetic_{}_m1_m2_performance_results.p'.format(pattern)\n",
    "#file_name = 'tmp.p'\n",
    "#pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Landscape of Non-convex method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_run_results(n1 = 50, n2 = 50, mean_M = 1, num_experiment=1, sigma = 0.1, pattern = 'block'):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'OLS', 'PCA', 'non_convex']\n",
    "\n",
    "    if (pattern == 'block' or pattern == 'stagger'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "\n",
    "\n",
    "    #r = 30\n",
    "\n",
    "    #np.random.seed(10)\n",
    "    #M0 = synthetic_M0(n1, n2, mean_M, r, type='Gamma')\n",
    "\n",
    "    n = n1\n",
    "\n",
    "    M0 = np.zeros((2*n, 2*n))\n",
    "    M0[0:n, 0:n] = -1 \n",
    "\n",
    "    Z = np.zeros((2*n, 2*n))\n",
    "    \n",
    "    Z[0:int(n/1.1), 0:int(n/1.1)] = 1\n",
    "    Z[n:, n:] = 1\n",
    "\n",
    "    # Z[0:n, 0:n] = 1\n",
    "    # Z[n:2*n, n:2*n] = 1\n",
    "\n",
    "    r = 1\n",
    "\n",
    "    u, s, vh = np.linalg.svd(M0, full_matrices=False)\n",
    "    u = u[:,:r]\n",
    "    vh = vh[:r, :]\n",
    "    print(u.T.dot(Z).dot(vh.T))\n",
    "    s1 = np.linalg.svd(projection_T_orthogonal(Z, M0), full_matrices=False, compute_uv=False)\n",
    "    print(s1)\n",
    "\n",
    "    #M0 = synthetic_M0(n1, n2, 0, r, type='Gaussian')\n",
    "    # M0 = readData.read_data('sales')\n",
    "    # n1 = M0.shape[0]\n",
    "    # n2 = M0.shape[1]\n",
    "    #r = 34\n",
    "\n",
    "    #E = np.random.normal(loc=0, scale=sigma, size=(n1, n2))\n",
    "\n",
    "    # s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "    # if (pattern == 'stagger'):\n",
    "    #     m1 = np.random.randint(low=1, high=n1)\n",
    "    #     m2 = np.random.randint(low=1, high=n2)\n",
    "    #     Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "    #     treat_units = []\n",
    "\n",
    "    # if (pattern == 'block'):\n",
    "    #     #m1 = np.random.randint(low=1, high=int(n1/10))\n",
    "    #     m1 = 10\n",
    "    #     m2 = np.random.randint(low=1, high=int(n2/3))\n",
    "    #     #m1 = 1\n",
    "    #     Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "\n",
    "    # if (pattern == 'adaptive'):\n",
    "    #     while True:\n",
    "    #         a = np.random.randint(40)+5\n",
    "    #         b = np.random.randint(40)+5\n",
    "    #         #b = 2\n",
    "    #         Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "    #         if (info == 'fail'):\n",
    "    #             continue\n",
    "    #         break\n",
    "    #     treat_units = []\n",
    "    #     m2 = 0\n",
    "    \n",
    "    # print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "    tau_star = 0\n",
    "    O = M0 + Z * 0\n",
    "    tau_list = np.arange(-2, 1, 0.01)\n",
    "    l = 8\n",
    "    datas = np.zeros((len(tau_list)))\n",
    "    datas_convex = np.zeros((len(tau_list)))\n",
    "    for (T,tau) in enumerate(tau_list): \n",
    "        u,s,vh = np.linalg.svd(O - tau*Z, full_matrices=False)\n",
    "        s_hard = s\n",
    "        s_hard[r:] = 0\n",
    "        M = (u*s_hard).dot(vh)\n",
    "        tau_new = np.sum(Z*(O-M)) / np.sum(Z)\n",
    "        datas[T] = np.sum((O-tau*Z-M)**2)\n",
    "\n",
    "        u,s,vh = np.linalg.svd(O - tau*Z, full_matrices=False)\n",
    "        s_soft = np.maximum(s-l, 0)\n",
    "        M = (u*s_soft).dot(vh)\n",
    "        datas_convex[T] = np.sum((O-tau*Z-M)**2) + 2*l*np.sum(s_soft)\n",
    "        #datas[T] = tau_new-tau\n",
    "\n",
    "\n",
    "    s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "    #print(s[0], np.linalg.norm(E, 2))\n",
    "    results = run_algo(['convex_debias'], O, Z, suggest_r = r, suggest_l = 4, eps = 1e-6, de_mean_O=False, treat_units=[], tau_star = 0, m2 = 0, M0 = M0, suggest_random_tau=0)\n",
    "    (M, tau) = results['convex_debias']\n",
    "    error_metric = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "    print(error_metric)\n",
    "    \n",
    "    plt.plot(tau_list, datas, label = r'$S(\\tau)$')\n",
    "    plt.plot(tau_list, datas_convex, label = r'$S_{\\mathrm{cvx}}(\\tau)$')\n",
    "    plt.xlabel(r'$\\tau$', fontsize=15)\n",
    "    #plt.ylabel(r'$S(\\tau)$', fontsize=15)\n",
    "    plt.legend(fontsize = 15)\n",
    "    plt.savefig('results/plot_results/instability_non_convex_optimization_Bai09.eps')\n",
    "\n",
    "\n",
    "tau_list = np.arange(-200, 201, 50)\n",
    "mean_M = 30\n",
    "n1 = 100\n",
    "n2 = 100\n",
    "sigma = 0\n",
    "pattern = 'block'\n",
    "datas = synthetic_experiment_performance_run_results(n1 = n1, n2 = n2, mean_M = mean_M, sigma = sigma, num_experiment = 1, pattern = pattern)\n",
    "\n",
    "# algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA', 'non_convex']\n",
    "# legend_dic = {'convex_debias': 'Debias Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC', 'non_convex': 'Non Convex'}\n",
    "\n",
    "# for algo in algo_list:\n",
    "#     plt.plot(tau_list, np.abs(datas[algo]), label = legend_dic[algo])\n",
    "# plt.legend()\n",
    "# plt.xlabel(r'$\\tau^{*}$', fontsize = 15)\n",
    "# plt.ylabel(r'$|\\tau-\\tau^{*}|$', fontsize = 15)\n",
    "# plt.savefig('invariance-tau.eps')\n",
    "#file_name = 'results/plot_results/synthetic_{}_m1_m2_performance_results.p'.format(pattern)\n",
    "#file_name = 'tmp.p'\n",
    "#pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_run_results(n1 = 50, n2 = 50, mean_M = 1, num_experiment=1, sigma = 0.1, pattern = 'block'):\n",
    "    samples = np.zeros(num_experiment)\n",
    "    t1 = time.time()\n",
    "\n",
    "    algo_list = ['convex_debias', 'missing', 'OLS', 'PCA', 'non_convex']\n",
    "\n",
    "    if (pattern == 'block' or pattern == 'stagger'):\n",
    "        algo_list.append('robust_synthetic_control')\n",
    "\n",
    "\n",
    "\n",
    "    #r = 30\n",
    "\n",
    "    #np.random.seed(10)\n",
    "    #M0 = synthetic_M0(n1, n2, mean_M, r, type='Gamma')\n",
    "\n",
    "    n = n1\n",
    "\n",
    "    M0 = np.zeros((2*n+int(1.5*n), 2*n+int(1.5*n)))\n",
    "    M0[0:n, 0:n] = 1\n",
    "    M0[n:2*n, n:2*n] = - 1 \n",
    "\n",
    "    Z = np.zeros((2*n+int(1.5*n), 2*n+int(1.5*n)))\n",
    "    \n",
    "    Z[0:int(n/1.1), 0:int(n/1.1)] = 1\n",
    "    Z[n:int(n/1.1)+n, n:int(n/1.1)+n] = 1\n",
    "\n",
    "    # Z[0:n, 0:n] = 1\n",
    "    # Z[n:2*n, n:2*n] = 1\n",
    "\n",
    "    Z[2*n:, 2*n:] = 1\n",
    "    r = 2\n",
    "\n",
    "    u, s, vh = np.linalg.svd(M0, full_matrices=False)\n",
    "    u = u[:,:r]\n",
    "    vh = vh[:r, :]\n",
    "    print(u.T.dot(Z).dot(vh.T))\n",
    "    s1 = np.linalg.svd(projection_T_orthogonal(Z, M0), full_matrices=False, compute_uv=False)\n",
    "    print(s1)\n",
    "\n",
    "    #M0 = synthetic_M0(n1, n2, 0, r, type='Gaussian')\n",
    "    # M0 = readData.read_data('sales')\n",
    "    # n1 = M0.shape[0]\n",
    "    # n2 = M0.shape[1]\n",
    "    #r = 34\n",
    "\n",
    "    #E = np.random.normal(loc=0, scale=sigma, size=(n1, n2))\n",
    "\n",
    "    # s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "\n",
    "    # if (pattern == 'stagger'):\n",
    "    #     m1 = np.random.randint(low=1, high=n1)\n",
    "    #     m2 = np.random.randint(low=1, high=n2)\n",
    "    #     Z = generate_Z(pattern_tuple=['stagger', (m1, m2)], M0=M0)\n",
    "    #     treat_units = []\n",
    "\n",
    "    # if (pattern == 'block'):\n",
    "    #     #m1 = np.random.randint(low=1, high=int(n1/10))\n",
    "    #     m1 = 10\n",
    "    #     m2 = np.random.randint(low=1, high=int(n2/3))\n",
    "    #     #m1 = 1\n",
    "    #     Z, treat_units = generate_Z(pattern_tuple=['block', (m1, m2)], M0=M0)\n",
    "\n",
    "    # if (pattern == 'adaptive'):\n",
    "    #     while True:\n",
    "    #         a = np.random.randint(40)+5\n",
    "    #         b = np.random.randint(40)+5\n",
    "    #         #b = 2\n",
    "    #         Z, info = generate_Z(pattern_tuple = ['adaptive', (a, b)], M0=M0)\n",
    "    #         if (info == 'fail'):\n",
    "    #             continue\n",
    "    #         break\n",
    "    #     treat_units = []\n",
    "    #     m2 = 0\n",
    "    \n",
    "    # print('***sparsity****', np.sum(Z) / np.size(Z))\n",
    "\n",
    "    tau_star = 0\n",
    "    O = M0 + Z * 0\n",
    "    tau_list = np.arange(-2, 2, 0.01)\n",
    "    l = 4\n",
    "    datas = np.zeros((len(tau_list)))\n",
    "    datas_convex = np.zeros((len(tau_list)))\n",
    "    for (T,tau) in enumerate(tau_list): \n",
    "        u,s,vh = np.linalg.svd(O - tau*Z, full_matrices=False)\n",
    "        s_hard = s\n",
    "        s_hard[r:] = 0\n",
    "        M = (u*s_hard).dot(vh)\n",
    "        tau_new = np.sum(Z*(O-M)) / np.sum(Z)\n",
    "        datas[T] = np.sum((O-tau*Z-M)**2)\n",
    "\n",
    "        u,s,vh = np.linalg.svd(O - tau*Z, full_matrices=False)\n",
    "        s_soft = np.maximum(s-l, 0)\n",
    "        M = (u*s_soft).dot(vh)\n",
    "        datas_convex[T] = np.sum((O-tau*Z-M)**2) + 2*l*np.sum(s_soft)\n",
    "        #datas[T] = tau_new-tau\n",
    "\n",
    "\n",
    "    s = np.linalg.svd(M0, full_matrices=False, compute_uv=False)\n",
    "    #print(s[0], np.linalg.norm(E, 2))\n",
    "    results = run_algo(['convex_debias'], O, Z, suggest_r = r, suggest_l = 4, eps = 1e-6, de_mean_O=False, treat_units=[], tau_star = 0, m2 = 0, M0 = M0, suggest_random_tau=0)\n",
    "    (M, tau) = results['convex_debias']\n",
    "    error_metric = metric_compute(M, tau, M0, tau_star, Z, ['tau_diff'])['tau_diff']\n",
    "    print(error_metric)\n",
    "    \n",
    "    plt.plot(tau_list, datas)\n",
    "    plt.plot(tau_list, datas_convex)\n",
    "    plt.xlabel(r'$\\tau$', fontsize=15)\n",
    "    plt.ylabel(r'$\\tilde{S}(\\tau)$', fontsize=15)\n",
    "    plt.savefig('results/plot_results/instability_non_convex_optimization.eps')\n",
    "\n",
    "\n",
    "tau_list = np.arange(-200, 201, 50)\n",
    "mean_M = 30\n",
    "n1 = 50\n",
    "n2 = 50\n",
    "sigma = 0\n",
    "pattern = 'block'\n",
    "datas = synthetic_experiment_performance_run_results(n1 = n1, n2 = n2, mean_M = mean_M, sigma = sigma, num_experiment = 1, pattern = pattern)\n",
    "\n",
    "# algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA', 'non_convex']\n",
    "# legend_dic = {'convex_debias': 'Debias Convex', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC', 'non_convex': 'Non Convex'}\n",
    "\n",
    "# for algo in algo_list:\n",
    "#     plt.plot(tau_list, np.abs(datas[algo]), label = legend_dic[algo])\n",
    "# plt.legend()\n",
    "# plt.xlabel(r'$\\tau^{*}$', fontsize = 15)\n",
    "# plt.ylabel(r'$|\\tau-\\tau^{*}|$', fontsize = 15)\n",
    "# plt.savefig('invariance-tau.eps')\n",
    "#file_name = 'results/plot_results/synthetic_{}_m1_m2_performance_results.p'.format(pattern)\n",
    "#file_name = 'tmp.p'\n",
    "#pickle.dump(datas, open(file_name, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_experiment_performance_plot_results(datas, label = 'min', file_name = ''):\n",
    "    results = np.abs(datas.values)\n",
    "    results_original = datas.values\n",
    "    display(datas.describe())\n",
    "    #display(datas.abs().describe())\n",
    "    display(datas.abs().describe())\n",
    "    if (label == 'min'):\n",
    "        results = (results.T / np.min(results, axis=1)).T\n",
    "        results = np.where(results > 10, 10, results)\n",
    "    else:\n",
    "        #results = (results.T / np.max(results, axis=1)).T\n",
    "        results = results\n",
    "\n",
    "    algo_list = ['convex_debias', 'robust_synthetic_control', 'missing', 'OLS', 'PCA']\n",
    "    legend_dic = {'convex_debias': 'Convex Debias', 'missing': 'MC-NNM', 'OLS':'OLS', 'PCA':'W-PCA', 'robust_synthetic_control':'RSC'}\n",
    "    color_dic = {'convex_debias': 'blue', 'missing': 'tab:red', 'OLS':'tab:green', 'PCA':'tab:purple', 'robust_synthetic_control':'gold'}\n",
    "    legend_list = []\n",
    "    for algo in algo_list:\n",
    "        if (algo not in datas.columns):\n",
    "            algo_list.remove(algo)\n",
    "    for algo in algo_list:\n",
    "        legend_list.append(legend_dic[algo])\n",
    "\n",
    "    columns = datas.columns\n",
    "    m = len(columns)\n",
    "    comparison_table = np.zeros((m,m))\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            ai = algo_list[i]\n",
    "            aj = algo_list[j]\n",
    "            comparison_table[i][j] = np.sum(datas.abs()[ai]<datas.abs()[aj]) / datas.shape[0]\n",
    "    df_compare = pd.DataFrame(data=comparison_table, index=legend_list, columns=legend_list)\n",
    "    display(df_compare)\n",
    "\n",
    "   \n",
    "    df_list = []\n",
    "    df_list_original = []\n",
    "    for i in range(results.shape[0]):\n",
    "        for algo in algo_list:\n",
    "            j = list(datas.columns).index(algo)\n",
    "            df_list.append([results[i,j], algo])\n",
    "\n",
    "            df_list_original.append([results_original[i,j], algo])\n",
    "\n",
    "    label = r'$|\\tau - \\tau^{*}|$'\n",
    "    df = pd.DataFrame(df_list, columns = [label, 'algos'])\n",
    "    df_original = pd.DataFrame(df_list_original, columns = [label, 'algos'])\n",
    "\n",
    "    #g = sns.displot(data = df, x = label, hue = 'algos', hue_order = algo_list, multiple = 'dodge', shrink = 0.8, legend=False, stat = 'probability', palette=color_dic)\n",
    "    #handles, labels = ax.get_legend_handles_labels()\n",
    "\n",
    "    \n",
    "\n",
    "    g = sns.displot(data = df, x = label, hue = 'algos', kind='ecdf', legend = False)\n",
    "\n",
    "    Ax = g.ax\n",
    "    Boxes = [item for item in Ax.get_children()\n",
    "        if isinstance(item, matplotlib.lines.Line2D)]\n",
    "\n",
    "    color_set = []\n",
    "    for item in Boxes:\n",
    "       if (item.get_color() in color_set):\n",
    "           pass\n",
    "       else:\n",
    "           color_set.append(item.get_color())\n",
    "    print(color_set)\n",
    "\n",
    "    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\n",
    "                 C, L in zip(color_set[::-1],\n",
    "                             legend_list)]\n",
    "\n",
    "    plt.legend(handles=legend_patches, fontsize = 13)\n",
    "    plt.ylabel('CDF', fontsize=14)\n",
    "    plt.xlabel(label, fontsize=14)\n",
    "    plt.xlim((0, 0.8))\n",
    "    plt.savefig(file_name + '_cdf.eps')\n",
    "    plt.show()\n",
    "\n",
    "    sns.catplot(data = df_original, x = 'algos', y = label, kind='bar')\n",
    "    plt.show()\n",
    "\n",
    "synthetic_experiment_performance_plot_results(datas, label = 'max', file_name = file_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
